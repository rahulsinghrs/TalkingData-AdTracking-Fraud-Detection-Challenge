{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most_freq_hours_in_test_data    = [4, 5, 9, 10, 13, 14]\n",
    "# middle1_freq_hours_in_test_data = [16, 17, 22] \n",
    "# least_freq_hours_in_test_data   = [6, 11, 15]\n",
    "# num_leaves :  7  ->  9\n",
    "# max_depth  :  4  ->  5\n",
    "# subsample  : 0.7 -> 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # for validation \n",
    "import lightgbm as lgb\n",
    "import gc # memory \n",
    "from datetime import datetime # train time checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "VALIDATE = False\n",
    "RANDOM_STATE = 50\n",
    "VALID_SIZE = 0.90\n",
    "MAX_ROUNDS = 1000\n",
    "EARLY_STOP = 50\n",
    "OPT_ROUNDS = 650\n",
    "skiprows = range(1,109903891)\n",
    "nrows = 75000000\n",
    "output_filename = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cols = ['ip','app','device','os', 'channel', 'click_time', 'is_attributed']\n",
    "train_df = pd.read_csv('/Users/rahul/Desktop/TalkingData AdTracking Fraud Detection Challenge/Data/mnt-2/ssd/kaggle-talkingdata2/competition_files/train.csv', skiprows=skiprows, nrows=nrows,dtype=dtypes, usecols=train_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_train = len(train_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_freq_hours_in_test_data    = [4, 5, 9, 10, 13, 14]\n",
    "middle1_freq_hours_in_test_data = [16, 17, 22]\n",
    "least_freq_hours_in_test_data   = [6, 11, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_data( df ):\n",
    "    \n",
    "    df['hour'] = pd.to_datetime(df.click_time).dt.hour.astype('uint8')\n",
    "    df['day'] = pd.to_datetime(df.click_time).dt.day.astype('uint8')\n",
    "    df.drop(['click_time'], axis=1, inplace=True)\n",
    "    gc.collect()\n",
    "    \n",
    "    df['in_test_hh'] = (   4 \n",
    "                         - 3*df['hour'].isin(  most_freq_hours_in_test_data ) \n",
    "                         - 2*df['hour'].isin(  middle1_freq_hours_in_test_data ) \n",
    "                         - 1*df['hour'].isin( least_freq_hours_in_test_data ) ).astype('uint8')\n",
    "    gp = df[['ip', 'day', 'in_test_hh', 'channel']].groupby(by=['ip', 'day', 'in_test_hh'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_day_test_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','in_test_hh'], how='left')\n",
    "    df.drop(['in_test_hh'], axis=1, inplace=True)\n",
    "    df['nip_day_test_hh'] = df['nip_day_test_hh'].astype('uint32')\n",
    "   \n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    gp = df[['ip', 'day', 'hour', 'channel']].groupby(by=['ip', 'day', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_day_hh'})\n",
    "    df = df.merge(gp, on=['ip','day','hour'], how='left')\n",
    "    df['nip_day_hh'] = df['nip_day_hh'].astype('uint16')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "    \n",
    "    gp = df[['ip', 'os', 'hour', 'channel']].groupby(by=['ip', 'os', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_os'})\n",
    "    df = df.merge(gp, on=['ip','os','hour'], how='left')\n",
    "    df['nip_hh_os'] = df['nip_hh_os'].astype('uint16')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    gp = df[['ip', 'app', 'hour', 'channel']].groupby(by=['ip', 'app',  'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_app'})\n",
    "    df = df.merge(gp, on=['ip','app','hour'], how='left')\n",
    "    df['nip_hh_app'] = df['nip_hh_app'].astype('uint16')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    gp = df[['ip', 'device', 'hour', 'channel']].groupby(by=['ip', 'device', 'hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'nip_hh_dev'})\n",
    "    df = df.merge(gp, on=['ip','device','hour'], how='left')\n",
    "    df['nip_hh_dev'] = df['nip_hh_dev'].astype('uint32')\n",
    "    del gp\n",
    "    gc.collect()\n",
    "\n",
    "    df.drop( ['ip','day'], axis=1, inplace=True )\n",
    "    gc.collect()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = prep_data(train_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric':'auc',\n",
    "          'learning_rate': 0.1,\n",
    "          'num_leaves': 9,  # we should let it be smaller than 2^(max_depth)\n",
    "          'max_depth': 5,  # -1 means no limit\n",
    "          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "          'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "          'subsample': 0.9,  # Subsample ratio of the training instance.\n",
    "          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n",
    "          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "          'nthread': 8,\n",
    "          'verbose': 0,\n",
    "          'scale_pos_weight':99.7, # because training data is extremely unbalanced \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh', 'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahul/anaconda3/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain's auc: 0.969659\n",
      "[100]\ttrain's auc: 0.973444\n",
      "[150]\ttrain's auc: 0.974863\n",
      "[200]\ttrain's auc: 0.975651\n",
      "[250]\ttrain's auc: 0.976331\n",
      "[300]\ttrain's auc: 0.976867\n",
      "[350]\ttrain's auc: 0.977299\n",
      "[400]\ttrain's auc: 0.977682\n",
      "[450]\ttrain's auc: 0.978017\n",
      "[500]\ttrain's auc: 0.978316\n",
      "[550]\ttrain's auc: 0.978586\n",
      "[600]\ttrain's auc: 0.978834\n",
      "[650]\ttrain's auc: 0.97908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if VALIDATE:\n",
    "\n",
    "    train_df, val_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )\n",
    "    dtrain = lgb.Dataset(train_df[predictors].values, \n",
    "                         label=train_df[target].values,\n",
    "                         feature_name=predictors,\n",
    "                         categorical_feature=categorical)\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    dvalid = lgb.Dataset(val_df[predictors].values,\n",
    "                         label=val_df[target].values,\n",
    "                         feature_name=predictors,\n",
    "                         categorical_feature=categorical)\n",
    "    del val_df\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    model = lgb.train(params, \n",
    "                      dtrain, \n",
    "                      valid_sets=[dtrain, dvalid], \n",
    "                      valid_names=['train','valid'], \n",
    "                      evals_result=evals_results, \n",
    "                      num_boost_round=MAX_ROUNDS,\n",
    "                      early_stopping_rounds=EARLY_STOP,\n",
    "                      verbose_eval=50, \n",
    "                      feval=None)\n",
    "\n",
    "    del dvalid\n",
    "\n",
    "else:\n",
    "\n",
    "    gc.collect()\n",
    "    dtrain = lgb.Dataset(train_df[predictors].values, label=train_df[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical\n",
    "                          )\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    model = lgb.train(params, \n",
    "                      dtrain, \n",
    "                      valid_sets=[dtrain], \n",
    "                      valid_names=['train'], \n",
    "                      evals_result=evals_results, \n",
    "                      num_boost_round=OPT_ROUNDS,\n",
    "                      verbose_eval=50,\n",
    "                      feval=None)\n",
    "    \n",
    "del dtrain\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cols = ['ip','app','device','os', 'channel', 'click_time', 'click_id']\n",
    "test_df = pd.read_csv('/Users/rahul/Desktop/TalkingData AdTracking Fraud Detection Challenge/Data/test.csv', dtype=dtypes, usecols=test_cols)\n",
    "test_df = prep_data(test_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id']\n",
    "sub['is_attributed'] = model.predict(test_df[predictors])\n",
    "sub.to_csv(output_filename, index=False, float_format='%.9f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "============================ Final Report ============================\n",
      "======================================================================\n",
      "2018-04-17 19:59:44.805413 \n",
      "\n",
      "   train time     : 3:33:04.690565\n",
      "   output file    : submission.csv\n",
      "    train auc     : 0.97908\n",
      "   MAX_ROUNDS     : 1000\n",
      "   EARLY_STOP     : 50\n",
      "   OPT_ROUNDS     : 0\n",
      "\n",
      "    skiprows      : range(1, 109903891)\n",
      "      nrows       : 75000000\n",
      "\n",
      "    variables     : ['app', 'device', 'os', 'channel', 'hour', 'nip_day_test_hh', 'nip_day_hh', 'nip_hh_os', 'nip_hh_app', 'nip_hh_dev']\n",
      "   categorical    : ['app', 'device', 'os', 'channel', 'hour']\n",
      "\n",
      "  model params    : {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.1, 'num_leaves': 9, 'max_depth': 5, 'min_child_samples': 100, 'max_bin': 100, 'subsample': 0.9, 'subsample_freq': 1, 'colsample_bytree': 0.7, 'min_child_weight': 0, 'min_split_gain': 0, 'nthread': 8, 'verbose': 0, 'scale_pos_weight': 99.7, 'categorical_column': [0, 1, 2, 3, 4]}\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#adapted from:\n",
    "#https://www.kaggle.com/pranav84/single-lightgbm-in-r-with-75-mln-rows-lb-0-9690\n",
    "#https://www.kaggle.com/aharless/try-pranav-s-r-lgbm-in-python/code) \n",
    "print('=='*35)\n",
    "print('============================ Final Report ============================')\n",
    "print('=='*35)\n",
    "print(datetime.now(), '\\n')\n",
    "print('{:^17} : {:}'.format('train time', datetime.now()-start))\n",
    "print('{:^17} : {:}'.format('output file', output_filename))\n",
    "print('{:^17} : {:.5f}'.format('train auc', model.best_score['train']['auc']))\n",
    "if VALIDATE:\n",
    "    print('{:^17} : {:.5f}\\n'.format('valid auc', model.best_score['valid']['auc']))\n",
    "    print('{:^17} : {:}\\n{:^17} : {}\\n{:^17} : {}'.format('VALIDATE', VALIDATE, 'VALID_SIZE', VALID_SIZE, 'RANDOM_STATE', RANDOM_STATE))\n",
    "print('{:^17} : {:}\\n{:^17} : {}\\n{:^17} : {}\\n'.format('MAX_ROUNDS', MAX_ROUNDS, 'EARLY_STOP', EARLY_STOP, 'OPT_ROUNDS', model.best_iteration))\n",
    "print('{:^17} : {:}\\n{:^17} : {}\\n'.format('skiprows', skiprows, 'nrows', nrows))\n",
    "print('{:^17} : {:}\\n{:^17} : {}\\n'.format('variables', predictors, 'categorical', categorical))\n",
    "print('{:^17} : {:}\\n'.format('model params', params))\n",
    "print('=='*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
